# Preprocessing for Machine Learning
## Exploring missing data
###You've been given a dataset comprised of volunteer information from New York City, stored in the volunteer DataFrame. Explore the dataset using the plethora of methods and attributes pandas has to offer to answer the following question.
#How many missing values are in the locality column?
#Possible Answers
# 665
# 595 
# 70 <<---
# 35
'''
print(volunteer.isna().sum())
'''

##Droping Missing Data
###Now that you've explored the volunteer dataset and understand its structure and contents, it's time to begin dropping missing values.
###In this exercise, you'll drop both columns and rows to create a subset of the volunteer dataset.
###Instructions
###Drop the Latitude and Longitude columns from volunteer, storing as volunteer_cols.
###Subset volunteer_cols by dropping rows containing missing values in the category_desc, and store in a new variable called volunteer_subset.
###Take a look at the .shape attribute of volunteer_subset, to verify it worked correctly.

### Drop the Latitude and Longitude columns from volunteer
'''
volunteer_cols = volunteer.drop(["Latitude", "Longitude"], axis=1)
'''
### Drop rows with missing category_desc values from volunteer_cols
'''
volunteer_subset = volunteer_cols.dropna(subset=["category_desc"])
'''
### Print out the shape of the subset
'''
print(volunteer_subset.shape)
'''
#Exploring data types
###Taking another look at the dataset comprised of volunteer information from New York City, you want to know what types you'll be working with as you start to do more preprocessing.

###Which data types are present in the volunteer dataset?
'''
Floats and integers only
Integers only
Floats, integers, and objects <<------
Floats only
print(volunteer.dtypes)
'''
##Converting a column type
# If you take a look at the volunteer dataset types, you'll see that the column hits is type object. But, if you actually look at the column, you'll see that it consists of integers. Let's convert that column to type int.
#Instructions
#100 XP
#Take a look at the .head() of the hits column.
#Convert the hits column to type int.
#Take a look at the .dtypes of the dataset again, and notice that the column type has changed.

#Print the head of the hits column
print(volunteer["hits"].head())

#Convert the hits column to type int
volunteer["hits] = volunteer["hits"].astype(int)

#Look at the dtypes of the dataset
print(volunteer.dtypes)

##Class imbalance
#In the volunteer dataset, you're thinking about trying to predict the category_desc variable using the other features in the dataset. First, though, you need to know what the class distribution (and imbalance) is for that label.
#Which descriptions occur less than 50 times in the volunteer dataset?
#Possible Answers
# Emergency Preparedness
# Health
# Environment
# Environment and Emergency Preparedness  <<-----
# All of the above
print(volunteer.value_counts("category_desc")

## Stratified sampling
#You now know that the distribution of class labels in the category_desc column of the volunteer dataset is uneven. If you wanted to train a model to predict category_desc, you'll need to ensure that the model is trained on a sample of data that is representative of the entire dataset. Stratified sampling is a way to achieve this!
#Create a DataFrame of features, X, with all of the columns except category_desc.
#Create a DataFrame of labels, y from the category_desc column.
#Split X and y into training and test sets, ensuring that the class distribution in the labels is the same in both sets
#Print the labels and counts in y_train using .value_counts().

# Create a DataFrame with all columns except category_desc
X = volunteer.drop("category_desc", axis=1)

# Create a category_desc labels dataset
y = volunteer[["category_desc"]]

# Use stratified sampling to split up the dataset according to the y dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

#Note: stratify=y makes sure that the class labels are equally distributed in both training and test sets which is important for maintaining the same class distribution in both sets and helps evaluate the model performance more reliably
#Note: random_state=42 is a parameter to make sure that the split is reproducible (every time that it is ran, the same split will be made) this ensures consistency in testing and debugging

# Print the category_desc counts from y_train
print(y_train["category_desc"].value_counts())

###When to standardize
##Which is **not** a reason to standardize a dataset?
#A column you want to use for modeling has extremely high variance.
#You have a dataset with several continuous columns on different scales, and you'd like to use a linear model to train the data. 
#The models you're working with use some sort of distance metric in a linear space.
#Your dataset is comprised of categorical data.  <<---

##Modeling without normalizing
#Let's take a look at what might happen to your model's accuracy if you try to model data without doing some sort of standardization first.
#Here we have a subset of the wine dataset. One of the columns, Proline, has an extremely high variance compared to the other columns. This is an example of where a technique like log normalization would come in handy, which you'll learn about in the next section.
#The scikit-learn model training process should be familiar to you at this point, so we won't go too in-depth with it. You already have a k-nearest neighbors model available (knn) as well as the X and y sets you need to fit and score on.
#Instructions
#Split up the X and y sets into training and test sets, ensuring that class labels are equally distributed in both sets.
#Fit the knn model to the training features and labels.
#Print the test set accuracy of the knn model using the .score() method.

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)
knn = KNeighborsClassifier()

#Note: KNeighborsClassifier is an object created which is an instance of the k-nearest neighbors algorithm

# Fit the knn model to the training data
knn.fit(X_train, y_train)

#Note: the fit method is called on this object with X_train and y_train as arguments. This trains the k-nearest neighbors model using the training features (X_train) and the corresponding labels (y_trains)

# Score the model on the test data
print(knn.score(X_test, y_test))

#Note: the score method is used to evaluate the model's accuracy of the model of the test set comparing predicted labels for X_test with actualy labels y_test and the results printed gives the accuracy of the model on the test data. This score indicates how well the model is performing on unseen data, which is a crucial step in assessing the model's generalization capability

##Check the variance
#Check the variance of the columns in the wine dataset. Out of the four columns listed, which column is the most appropriate candidate for normalization?
#Possible Answers:
#Alcohol
#Proline <<---
#Proanthocyanins
#Ash
print(wine.var())

##Log normalizing in Python
#Now that we know that the Proline column in our wine dataset has a large amount of variance, let's log normalize it.
#numpy has been imported as np.
#Instructions
# Print out the variance of the Proline column for reference.
#Use the np.log() function on the Proline column to create a new, log-normalized column named Proline_log.
#Print out the variance of the Proline_log column to see the difference.

# Print out the variance of the Proline column
print(wine["Proline"].var())

# Apply the log normalization function to the Proline column
wine["Proline_log"] = np.log(wine["Proline"])

# Check the variance of the normalized Proline column
print(wine.Proline_log.var())

##Sclaling data - investigating columns
#You want to use the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset to train a linear model, but it's possible that these columns are all measured in different ways, which would bias a linear model.
#Which of the following statements about these columns is true?
#Possible answers
#The standard deviation of Alcohol is greater than the standard deviation of Malic acid.
#The standard deviations of Ash and Alcalinity of ash are equal.
#The max of Ash is 3.23, the max of Alcalinity of ash is 30, and the max of Magnesium is 162. <<---
#The mean Malic acid is greater than the mean Ash.
wine[["Ash", "Alcalinity of ash", "Magnesium"]].describe()

#Since we know that the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset are all on different scales, let's standardize them in a way that allows for use in a linear model.
#Instructions
#Import the StandardScaler class.
#Instantiate a StandardScaler() and store it in the variable, scaler.
#Create a subset of the wine DataFrame containing the Ash, Alcalinity of ash, and Magnesium columns, assign it to wine_subset.
#Fit and transform the standard scaler to wine_subset.

# Import StandardScaler
from sklearn.preprocessing import StandardScaler

# Create the scaler
scaler = StandardScaler()

#Note: StandardScaler is from skitlearn and will be used to standardize the features by removing the mean and scaling the unit variance. This is a common preprocessing step for many machine learning algorithms, including k-nearest neighbors, as it ensures that all features contribute equally to the distance calculations

# Subset the DataFrame you want to scale 
wine_subset = wine[["Ash", "Alcalinity of ash", "Magnesium"]]

# Apply the scaler to wine_subset
wine_subset_scaled = scaler.fit_transform(wine_subset)

##KNN on non-scaled data
#Before adding standardization to your scikit-learn workflow, you'll first take a look at the accuracy of a K-nearest neighbors model on the wine dataset without standardizing the data.
#The knn model as well as the X and y data and labels sets have been created already.
#Instructions
#Split the dataset into training and test sets.
#Fit the knn model to the training data.
#Print out the test set accuracy of your trained knn model.

# Split the dataset and labels into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

# Fit the k-nearest neighbors model to the training data
knn.fit(X_train, y_train)

# Score the model on the test data
print(knn.score(X_test, y_test))

##KNN on scaled data
#The accuracy score on the unscaled wine dataset was decent, but let's see what you can achieve by using standardization. Once again, the knn model as well as the X and y data and labels set have already been created for you.
#Instructions
#Create the StandardScaler() method, stored in a variable named scaler.
#Scale the training and test features, being careful not to introduce data leakage.
#Fit the knn model to the scaled training data.
#Evaluate the model's performance by computing the test set accuracy.

# Instantiate a StandardScaler
scaler = StandardScaler()

# Scale the training and test features
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Fit the k-nearest neighbors model to the training data
knn.fit(X_train_scaled, y_train)

# Score the model on the test data
print(knn.score(X_test_scaled, y_test))

## Feature Engineering
#Which of the following examples are good candidates for creating new features?
#A column of timestamps
#A column of newspaper headlines
#A column of weight measurements
#1 and 2 <<---
# None of the above
#Note: timestamps can be broken into days or months, and headlines can be used for natural language processing

## Identifying areas for feature engineering
#Take an exploratory look at the volunteer dataset.
#Which of the following columns would you want to perform a feature engineering task on?
#vol_requests
#title
#created_date
#category_desc
#2, 3, and 4  <<---
volunteer[['vol_requests','title','created_date','category_desc']].head()

##Encoding categorical variables - binary
#Take a look at the hiking dataset. There are several columns here that need encoding before they can be modeled, one of which is the Accessible column. Accessible is a binary feature, so it has two values, Y or N, which need to be encoded into 1's and 0's. Use scikit-learn's LabelEncoder method to perform this transformation.
#Instructions
#Store LabelEncoder() in a variable named enc.
#Using the encoder's .fit_transform() method, encode the hiking dataset's "Accessible" column. Call the new column Accessible_enc.
#Compare the two columns side-by-side to see the encoding.
# Set up the LabelEncoder object
enc = LabelEncoder()

# Apply the encoding to the "Accessible" column
hiking["Accessible_enc"] = enc.fit_transform(hiking["Accessible"])

# Compare the two columns
print(hiking[["Accessible", "Accessible_enc"]].head())

## Encoding categorical variables - one-hot
#One of the columns in the volunteer dataset, category_desc, gives category descriptions for the volunteer opportunities listed. Because it is a categorical variable with more than two categories, we need to use one-hot encoding to transform this column numerically. Use pandas' pd.get_dummies() function to do so.
#Instructions
#Call get_dummies() on the volunteer["category_desc"] column to create the encoded columns and assign it to category_enc.
#Print out the .head() of the category_enc variable to take a look at the encoded columns.

# Transform the category_desc column
category_enc = pd.get_dummies(volunteer["category_desc"])

# Take a look at the encoded columns
print(category_enc.head())

##Aggregating numerical features
#A good use case for taking an aggregate statistic to create a new feature is when you have many features with similar, related values. Here, you have a DataFrame of running times named running_times_5k. For each name in the dataset, take the mean of their 5 run times.
#Instructions
#Use the .loc[] method to select all rows and columns to find the .mean() of the each columns.
#Print the .head() of the DataFrame to see the mean column.

# Use .loc to create a mean column
running_times_5k["mean"] = running_times_5k.loc[:, "run1":"run5"].mean(axis=1)

# Take a look at the results
print(running_times_5k.head())

##Extracting datetime components
#There are several columns in the volunteer dataset comprised of datetimes. Let's take a look at the start_date_date column and extract just the month to use as a feature for modeling.
#Instructions
#Convert the start_date_date column into a pandas datetime column and store it in a new column called start_date_converted.
#Retrieve the month component of start_date_converted and store it in a new column called start_date_month.
#Print the .head() of just the start_date_converted and start_date_month columns.

# First, convert string column to date column
volunteer["start_date_converted"] = pd.to_datetime(volunteer["start_date_date"])

# Extract just the month from the converted column
volunteer["start_date_month"] = volunteer["start_date_converted"].dt.month

# Take a look at the converted and new month columns
print(volunteer[["start_date_converted", "start_date_month"]].head())

##Extracting string patterns
#The Length column in the hiking dataset is a column of strings, but contained in the column is the mileage for the hike. We're going to extract this mileage using regular expressions, and then use a lambda in pandas to apply the extraction to the DataFrame.
#Instructions
#Search the text in the length argument for numbers and decimals using an appropriate pattern.
#Extract the matched pattern and convert it to a float.
#Apply the return_mileage() function to each row in the hiking["Length"] column.

## Write a pattern to extract numbers and decimals
def return_mileage(length):
    
    # Search the text for matches
    mile = re.search("\d+\.\d+", length)
#Note:Pattern Matching: Inside the function, the re.search("\d+\.\d+", length) is used. This line utilizes a regular expression to search for patterns in the length argument. The pattern \d+\.\d+ is a regular expression that matches sequences of digits that include a decimal point. Specifically:
#Note: \d+ matches one or more digits.
#Note: \. matches the decimal point.
#Note: \d+ again matches one or more digits following the decimal point. This pattern effectively captures numbers with decimals, which are likely to represent mileage in the context of the exercise.
    # If a value is returned, use group(0) to return the found value
    if mile is not None:
        return float(mile.group(0))
        
# Apply the function to the Length column and take a look at both columns
hiking["Length_num"] = hiking["Length"].apply(return_mileage)
print(hiking[["Length", "Length_num"]].head())

##Vectoruzing text
#You'll now transform the volunteer dataset's title column into a text vector, which you'll use in a prediction task in the next exercise
#Instructions
#Store the volunteer["title"] column in a variable named title_text.
#Instantiate a TfidfVectorizer as tfidf_vec.
#Transform the text in title_text into a tf-idf vector using tfidf_vec.

# Take the title text
title_text = volunteer["title"]

# Create the vectorizer method
tfidf_vec = TfidfVectorizer()

# Transform the text into tf-idf vectors
text_tfidf = tfidf_vec.fit_transform(title_text)

##Text classification using tf/idf vectors
#Now that you've encoded the volunteer dataset's title column into tf/idf vectors, you'll use those vectors to predict the category_desc column.
#Instructions
#Split the text_tfidf vector and y target variable into training and test sets, setting the stratify parameter equal to y, since the class distribution is uneven. Notice that we have to run the .toarray() method on the tf/idf vector, in order to get in it the proper format for scikit-learn.
#Fit the X_train and y_train data to the Naive Bayes model, nb.
#Print out the test set accuracy.

# Split the dataset according to the class distribution of category_desc
y = volunteer["category_desc"]
X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y, random_state=42)

# Fit the model to the training data
nb.fit(X_train, y_train)

# Print out the model's accuracy
print(nb.score(X_test, y_test))
