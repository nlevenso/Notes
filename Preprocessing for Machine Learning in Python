# Preprocessing for Machine Learning
## Exploring missing data
###You've been given a dataset comprised of volunteer information from New York City, stored in the volunteer DataFrame. Explore the dataset using the plethora of methods and attributes pandas has to offer to answer the following question.
#How many missing values are in the locality column?
#Possible Answers
# 665
# 595 
# 70 <<---
# 35
'''
print(volunteer.isna().sum())
'''

##Droping Missing Data
###Now that you've explored the volunteer dataset and understand its structure and contents, it's time to begin dropping missing values.
###In this exercise, you'll drop both columns and rows to create a subset of the volunteer dataset.
###Instructions
###Drop the Latitude and Longitude columns from volunteer, storing as volunteer_cols.
###Subset volunteer_cols by dropping rows containing missing values in the category_desc, and store in a new variable called volunteer_subset.
###Take a look at the .shape attribute of volunteer_subset, to verify it worked correctly.

### Drop the Latitude and Longitude columns from volunteer
'''
volunteer_cols = volunteer.drop(["Latitude", "Longitude"], axis=1)
'''
### Drop rows with missing category_desc values from volunteer_cols
'''
volunteer_subset = volunteer_cols.dropna(subset=["category_desc"])
'''
### Print out the shape of the subset
'''
print(volunteer_subset.shape)
'''
#Exploring data types
###Taking another look at the dataset comprised of volunteer information from New York City, you want to know what types you'll be working with as you start to do more preprocessing.

###Which data types are present in the volunteer dataset?
'''
Floats and integers only
Integers only
Floats, integers, and objects <<------
Floats only
print(volunteer.dtypes)
'''
##Converting a column type
# If you take a look at the volunteer dataset types, you'll see that the column hits is type object. But, if you actually look at the column, you'll see that it consists of integers. Let's convert that column to type int.
#Instructions
#100 XP
#Take a look at the .head() of the hits column.
#Convert the hits column to type int.
#Take a look at the .dtypes of the dataset again, and notice that the column type has changed.

#Print the head of the hits column
print(volunteer["hits"].head())

#Convert the hits column to type int
volunteer["hits] = volunteer["hits"].astype(int)

#Look at the dtypes of the dataset
print(volunteer.dtypes)

##Class imbalance
#In the volunteer dataset, you're thinking about trying to predict the category_desc variable using the other features in the dataset. First, though, you need to know what the class distribution (and imbalance) is for that label.
#Which descriptions occur less than 50 times in the volunteer dataset?
#Possible Answers
# Emergency Preparedness
# Health
# Environment
# Environment and Emergency Preparedness  <<-----
# All of the above
print(volunteer.value_counts("category_desc")

## Stratified sampling
#You now know that the distribution of class labels in the category_desc column of the volunteer dataset is uneven. If you wanted to train a model to predict category_desc, you'll need to ensure that the model is trained on a sample of data that is representative of the entire dataset. Stratified sampling is a way to achieve this!
#Create a DataFrame of features, X, with all of the columns except category_desc.
#Create a DataFrame of labels, y from the category_desc column.
#Split X and y into training and test sets, ensuring that the class distribution in the labels is the same in both sets
#Print the labels and counts in y_train using .value_counts().

# Create a DataFrame with all columns except category_desc
X = volunteer.drop("category_desc", axis=1)

# Create a category_desc labels dataset
y = volunteer[["category_desc"]]

# Use stratified sampling to split up the dataset according to the y dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

# Print the category_desc counts from y_train
print(y_train["category_desc"].value_counts())
